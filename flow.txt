# ğŸ”„ RAG GenAI Assistant - System Flow & Architecture

## ğŸ“‹ Overall System Flow

```
User Input â†’ Frontend â†’ API Endpoint â†’ RAG Pipeline â†’ LLM API â†’ Response â†’ Frontend â†’ User
```

## ğŸ—ï¸ Detailed Architecture Flow

### 1. **Initialization Phase**
```
Start Application
â”œâ”€â”€ Load docs.json
â”œâ”€â”€ Chunk documents (300-500 tokens)
â”œâ”€â”€ Generate embeddings for each chunk
â”œâ”€â”€ Store embeddings in vector database
â””â”€â”€ Start API server
```

### 2. **Chat Request Flow**
```
User sends message
â”œâ”€â”€ Frontend: Capture input + sessionId
â”œâ”€â”€ API: POST /api/chat
â”‚   â”œâ”€â”€ Validate request
â”‚   â”œâ”€â”€ Retrieve conversation history (3-5 pairs)
â”‚   â”œâ”€â”€ Generate query embedding
â”‚   â”œâ”€â”€ Perform similarity search
â”‚   â”‚   â”œâ”€â”€ Calculate cosine similarity
â”‚   â”‚   â”œâ”€â”€ Filter by threshold
â”‚   â”‚   â””â”€â”€ Get top 3 chunks
â”‚   â”œâ”€â”€ Construct prompt
â”‚   â”‚   â”œâ”€â”€ System instructions
â”‚   â”‚   â”œâ”€â”€ Retrieved context
â”‚   â”‚   â”œâ”€â”€ Conversation history
â”‚   â”‚   â””â”€â”€ User question
â”‚   â”œâ”€â”€ Call LLM API
â”‚   â”‚   â”œâ”€â”€ Send structured prompt
â”‚   â”‚   â”œâ”€â”€ Handle errors/retries
â”‚   â”‚   â””â”€â”€ Log token usage
â”‚   â”œâ”€â”€ Update conversation history
â”‚   â””â”€â”€ Return response
â””â”€â”€ Frontend: Display response
```

## ğŸ“Š Data Flow Diagram

### Document Processing Flow
```
docs.json â†’ Document Parser â†’ Chunker â†’ Embedding Generator â†’ Vector Store
     â”‚              â”‚            â”‚               â”‚                â”‚
     â”‚              â”‚            â”‚               â”‚                â”‚
     â””â”€â”€ Raw Text â”€â”€â”´â”€â”€ Chunks â”€â”€â”´â”€â”€ Vectors â”€â”€â”€â”€â”€â”´â”€â”€ Indexed Data
```

### Query Processing Flow
```
User Query â†’ Query Embedding â†’ Similarity Search â†’ Context Retrieval â†’ Prompt Construction â†’ LLM â†’ Response
     â”‚              â”‚               â”‚                   â”‚                  â”‚           â”‚
     â”‚              â”‚               â”‚                   â”‚                  â”‚           â”‚
     â””â”€â”€ Text â”€â”€â”€â”€â”€â”€â”´â”€â”€ Vector â”€â”€â”€â”€â”€â”´â”€â”€ Relevant Chunks â”€â”€â”´â”€â”€ Context + History â”€â”€â”´â”€â”€ Answer
```

## ğŸ”§ Component Interactions

### 1. **Document Management Module**
```
DocumentManager
â”œâ”€â”€ loadDocuments() â†’ docs.json
â”œâ”€â”€ chunkDocuments(text, chunkSize=300-500)
â”œâ”€â”€ generateEmbeddings(chunks)
â””â”€â”€ storeInVectorDB(embeddings)
```

### 2. **RAG Pipeline Module**
```
RAGPipeline
â”œâ”€â”€ generateQueryEmbedding(query)
â”œâ”€â”€ similaritySearch(queryEmbedding, topK=3)
â”œâ”€â”€ filterByThreshold(results, threshold=0.7)
â”œâ”€â”€ constructPrompt(context, history, question)
â””â”€â”€ handleLowSimilarity()
```

### 3. **LLM Integration Module**
```
LLMService
â”œâ”€â”€ callLLM(prompt, temperature=0-0.3)
â”œâ”€â”€ handleAPIErrors()
â”œâ”€â”€ logTokenUsage()
â””â”€â”€ retryMechanism()
```

### 4. **Conversation Management Module**
```
ConversationManager
â”œâ”€â”€ getHistory(sessionId, limit=3-5)
â”œâ”€â”€ addToHistory(sessionId, message, response)
â”œâ”€â”€ maintainContext()
â””â”€â”€ sessionHandling()
```

## ğŸ”„ Request-Response Cycle

### API Request Flow
```
POST /api/chat
{
  "sessionId": "abc123",
  "message": "How can I reset my password?"
}

â†“ Processing Steps â†“

1. Validate input
2. Get conversation history for sessionId
3. Generate embedding for user message
4. Search vector database for similar chunks
5. Apply similarity threshold
6. If similarity < threshold â†’ Return fallback
7. Construct prompt with context + history
8. Call LLM API
9. Update conversation history
10. Return response
```

### API Response Flow
```
{
  "reply": "Users can reset their password from Settings > Security.",
  "tokensUsed": 120,
  "retrievedChunks": 3,
  "similarityScores": [0.85, 0.82, 0.78] // Optional
}
```

## ğŸ—‚ï¸ File Structure & Data Flow

### Project Structure
```
assignment/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js (or app.py)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ documentManager.js
â”‚   â”‚   â”œâ”€â”€ ragPipeline.js
â”‚   â”‚   â”œâ”€â”€ llmService.js
â”‚   â”‚   â””â”€â”€ conversationManager.js
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ docs.json
â”‚   â”‚   â””â”€â”€ vectorStore.json (or SQLite DB)
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ chat.js
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ App.js
â”‚   â””â”€â”€ package.json
â””â”€â”€ flow.txt
```

### Data Storage Flow
```
docs.json â†’ Processing â†’ Vector Store â†’ Query â†’ Results â†’ Context â†’ LLM â†’ Response
    â”‚           â”‚           â”‚          â”‚        â”‚        â”‚        â”‚
    â”‚           â”‚           â”‚          â”‚        â”‚        â”‚        â”‚
    â””â”€â”€ Raw â”€â”€â”€â”€â”´â”€â”€ Chunks â”€â”€â”´â”€â”€ Vectors â”€â”´â”€â”€ Search â”€â”´â”€â”€ Prompt â”€â”´â”€â”€ Answer
```

## âš¡ Performance Considerations

### Optimization Points
1. **Embedding Caching**: Cache frequently used embeddings
2. **Batch Processing**: Process multiple chunks together
3. **Similarity Threshold**: Tune for optimal retrieval
4. **Context Window**: Limit conversation history size
5. **Error Handling**: Graceful degradation on failures

### Memory Management
```
Vector Store (In-memory/SQLite)
â”œâ”€â”€ Document embeddings
â”œâ”€â”€ Metadata (title, chunk_id)
â”œâ”€â”€ Similarity index
â””â”€â”€ Conversation sessions
```

## ğŸš¨ Error Handling Flow

### Error Scenarios
```
API Errors
â”œâ”€â”€ Invalid input â†’ 400 Bad Request
â”œâ”€â”€ No documents â†’ 500 Server Error
â”œâ”€â”€ LLM API failure â†’ Fallback response
â”œâ”€â”€ Low similarity â†’ "I don't have enough information"
â””â”€â”€ Rate limit â†’ Retry with exponential backoff
```

### Fallback Mechanism
```
If similarity < threshold OR LLM fails:
â”œâ”€â”€ Return: "I don't have enough information to answer this question."
â”œâ”€â”€ Log the issue
â”œâ”€â”€ Suggest rephrasing
â””â”€â”€ Maintain conversation flow
```

## ğŸ¯ Success Criteria Flow

### What Makes It Work
1. âœ… Real embedding-based retrieval (not keyword search)
2. âœ… Proper document chunking and storage
3. âœ… Similarity threshold implementation
4. âœ… Context-aware responses
5. âœ… Conversation history management
6. âœ… Error handling and fallbacks
7. âœ… Token usage tracking
8. âœ… Clean API interface

### Testing Flow
```
Unit Tests
â”œâ”€â”€ Document chunking
â”œâ”€â”€ Embedding generation
â”œâ”€â”€ Similarity search
â”œâ”€â”€ Prompt construction
â””â”€â”€ API endpoints

Integration Tests
â”œâ”€â”€ End-to-end RAG pipeline
â”œâ”€â”€ LLM API integration
â”œâ”€â”€ Conversation management
â””â”€â”€ Error scenarios

User Acceptance Tests
â”œâ”€â”€ Query accuracy
â”œâ”€â”€ Response relevance
â”œâ”€â”€ UI/UX experience
â””â”€â”€ Performance under load
```

This flow provides the complete roadmap for building the RAG GenAI assistant module by module.
